{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About the Data Set:**\n",
    "\n",
    "There are 8 different text files of ebooks which are available freely on http://www.gutenberg.org/ . The books are\n",
    "1. The Adventures of Tom Sawyer\n",
    "2. The Time Machine\n",
    "3. The War of the Worlds\n",
    "4. Astounding Stories\n",
    "5. Common Science\n",
    "6. Northanger Abby\n",
    "7. General Science\n",
    "8. Sailing Alone Around the World\n",
    "\n",
    "**Steps Performed:**\n",
    "\n",
    "- Importing of text files to python\n",
    "- Text PArsing and transformation operations are performed such as lower case conversion, removal of special characters, contraction words, tokenizing etc.\n",
    "- Tagging parts of speech to each term\n",
    "- Stemming terms to get their root word\n",
    "- Stop Word Removal\n",
    "\n",
    "The project also shows the difference in the outcome when POS Tagging, Stop Word Removal and Stemming operations are not performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'TextFiles/TextFiles/'\n",
    "files = ['T1.txt', 'T2.txt', 'T3.txt', 'T4.txt', 'T5.txt', 'T6.txt','T7.txt', 'T8.txt']\n",
    "term_doc = []\n",
    "pos_tags = True\n",
    "stemming = True\n",
    "remove_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document T1.txt contains a total of 86484  terms.\n",
      "Document T1.txt contains 40039 terms after stemming.\n",
      "\n",
      "\n",
      "Document T2.txt contains a total of 108474  terms.\n",
      "Document T2.txt contains 48289 terms after stemming.\n",
      "\n",
      "\n",
      "Document T3.txt contains a total of 104778  terms.\n",
      "Document T3.txt contains 50177 terms after stemming.\n",
      "\n",
      "\n",
      "Document T4.txt contains a total of 83140  terms.\n",
      "Document T4.txt contains 35269 terms after stemming.\n",
      "\n",
      "\n",
      "Document T5.txt contains a total of 76238  terms.\n",
      "Document T5.txt contains 35030 terms after stemming.\n",
      "\n",
      "\n",
      "Document T6.txt contains a total of 35136  terms.\n",
      "Document T6.txt contains 15670 terms after stemming.\n",
      "\n",
      "\n",
      "Document T7.txt contains a total of 80206  terms.\n",
      "Document T7.txt contains 35461 terms after stemming.\n",
      "\n",
      "\n",
      "Document T8.txt contains a total of 64511  terms.\n",
      "Document T8.txt contains 29797 terms after stemming.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Initialize and Reading File\n",
    "for file in files:\n",
    "    with open (file_path+file, \"r\") as text_file:\n",
    "        adoc = text_file.read()\n",
    "    # Convert to all lower case - required\n",
    "    adoc = (\"%s\" %adoc).lower()\n",
    "\n",
    "    # Replace special characters with spaces\n",
    "    adoc = adoc.replace('-', ' ')\n",
    "    adoc = adoc.replace('_', ' ')\n",
    "    adoc = adoc.replace(',', ' ')\n",
    "\n",
    "    # Replace not contraction with not\n",
    "    adoc = adoc.replace(\"'nt\", \" not\")\n",
    "    adoc = adoc.replace(\"n't\", \" not\")\n",
    "    adoc = adoc.replace(\"'d\", \" \")\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(adoc)\n",
    "    tokens = [word.replace(',', '') for word in tokens]\n",
    "    tokens = [word for word in tokens if ('*' not in word) and word != \"''\" and word !=\"``\"]\n",
    "\n",
    "    for word in tokens:\n",
    "        word = re.sub(r'[^\\w\\d\\s]+','',word)\n",
    "    print(\"\\nDocument \"+file+\" contains a total of\", len(tokens), \" terms.\")\n",
    "    \n",
    "    #POS Tagging\n",
    "    if pos_tags:\n",
    "        tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Remove stop words\n",
    "    if remove_stop:\n",
    "        stop = stopwords.words('english') + list(string.punctuation)\n",
    "        stop.append(\"said\")\n",
    "        # Remove single character words and simple punctuation\n",
    "        tokens = [word for word in tokens if len(word) > 1]\n",
    "        # Remove stop words\n",
    "        if pos_tags:\n",
    "            tokens = [word for word in tokens if word[0] not in stop]\n",
    "            tokens = [word for word in tokens if (not word[0].replace('.','',1).isnumeric()) and word[0]!=\"'s\" ]\n",
    "        else:\n",
    "            tokens = [word for word in tokens if word not in stop]\n",
    "            tokens = [word for word in tokens if word != \"'s\" ]\n",
    "            \n",
    "    # Lemmatization - Stemming with POS\n",
    "    if stemming:\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        wn_tags = {'N':wn.NOUN, 'J':wn.ADJ, 'V':wn.VERB, 'R':wn.ADV}\n",
    "        wnl = WordNetLemmatizer()\n",
    "        stemmed_tokens = []\n",
    "        if pos_tags:\n",
    "            for token in tokens:\n",
    "                term = token[0]\n",
    "                pos = token[1]\n",
    "                pos = pos[0]\n",
    "                try:\n",
    "                    pos = wn_tags[pos]\n",
    "                    stemmed_tokens.append(wnl.lemmatize(term, pos=pos))\n",
    "                except:\n",
    "                    stemmed_tokens.append(stemmer.stem(term))\n",
    "        else:\n",
    "            for token in tokens:\n",
    "                stemmed_tokens.append(stemmer.stem(token))\n",
    "    if stemming:\n",
    "        print(\"Document \"+file+\" contains\", len(stemmed_tokens), \"terms after stemming.\\n\")\n",
    "        tokens = stemmed_tokens\n",
    "        \n",
    "    #Prepare Counts & Add to term_doc\n",
    "\n",
    "    #fdist = FreqDist(word for word in stemmed_tokens)\n",
    "    fdist = FreqDist(tokens)\n",
    "    # Use with Wordnet\n",
    "    td= {}\n",
    "    #term_doc = []\n",
    "    for word, freq in fdist.most_common(2000):\n",
    "        td[word] = freq\n",
    "    term_doc.append(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario: POS= True Remove Stop Words= True  Stemming= True\n",
      "------------------------------------------------------------\n",
      "TERM            TOTAL  D1   D2   D3   D4   D5   D6   D7   D8\n",
      "one             2127  291  437  348  211  312  121  202  205\n",
      "water           2040   47  922  825    7   94    7   55   83\n",
      "make            1928  204  694  262  185  237   63  169  114\n",
      "would           1855  270  407  195  309  222   60  289  103\n",
      "go              1620  212  292   18  239  154  103  374  228\n",
      "come            1511  211  153   62  126  276  155  282  246\n",
      "could           1363  221  121   49  364  195   93  203  117\n",
      "time            1333  137  128  175  167  164  213  216  133\n",
      "see             1188  179  232  129  156  110   72  172  138\n",
      "light           1175   87  461  322   21   92   61   60   71\n",
      "get             1146  171  291   24   76  121   53  315   95\n",
      "air             1126   69  518  412   20   19   23   30   35\n",
      "know            1042  165  102  112  223  119   46  202   73\n",
      "day              939   87   52   82  117  337   49  107  108\n",
      "take             926  129  174   82  110  135   52  178   66\n",
      "upon             891  168   11  163   88   28  113  148  172\n",
      "way              844   78  211  122   73  100   42  116  102\n",
      "thing            832  120  241   10   76   57  100  113  115\n",
      "like             828  173  119   56  100   80   74  130   96\n",
      "man              827  248   38  104   90   90   70   62  125\n",
      "____________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Prepare Term-Document Matrix\n",
    "\n",
    "td_mat = {}\n",
    "for td in term_doc:\n",
    "    td_mat = Counter(td_mat)+Counter(td)\n",
    "td_matrix = {}\n",
    "for k, v in td_mat.items():\n",
    "    td_matrix[k] = [v]\n",
    "\n",
    "for td in term_doc:\n",
    "    for k, v in td_matrix.items():\n",
    "        if k in td:\n",
    "            td_matrix[k].append(td[k])\n",
    "        else:\n",
    "            td_matrix[k].append(0)\n",
    "                \n",
    "#Print Term Document Matrix\n",
    "\n",
    "td_matrix_sorted = sorted(td_matrix.items(), key=operator.itemgetter(1),reverse=True)\n",
    "print(\"Scenario: POS=\", pos_tags, \"Remove Stop Words=\", remove_stop, \" Stemming=\", stemming)\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"TERM            TOTAL  D1   D2   D3   D4   D5   D6   D7   D8\")\n",
    "for i in range(20):\n",
    "    s = '{:<15s}'.format(td_matrix_sorted[i][0])\n",
    "    v = td_matrix_sorted[i][1]\n",
    "    #print(v)\n",
    "    for j in range(9):\n",
    "        s = s + '{:>5d}'.format(v[j])\n",
    "    print('{:<60s}'.format(s))\n",
    "print(\"____________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading https://files.pythonhosted.org/packages/23/4e/1254d26ce5d36facdcbb5820e7e434328aed68e99938c75c9d4e2fee5efb/wordcloud-1.5.0-cp37-cp37m-win_amd64.whl (153kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\vasuk\\anaconda3\\lib\\site-packages (from wordcloud) (1.15.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\vasuk\\anaconda3\\lib\\site-packages (from wordcloud) (5.3.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above matrix shows the maximum occuring terms based on filtering done by POS, Stemming and Stop word removal.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 2: When POS Tagging, Stemming, Stop Word Remocal Operations are not performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document T1.txt contains a total of 86484  terms.\n",
      "\n",
      "Document T2.txt contains a total of 108474  terms.\n",
      "\n",
      "Document T3.txt contains a total of 104778  terms.\n",
      "\n",
      "Document T4.txt contains a total of 83140  terms.\n",
      "\n",
      "Document T5.txt contains a total of 76238  terms.\n",
      "\n",
      "Document T6.txt contains a total of 35136  terms.\n",
      "\n",
      "Document T7.txt contains a total of 80206  terms.\n",
      "\n",
      "Document T8.txt contains a total of 64511  terms.\n",
      "Scenario: POS= False Remove Stop Words= False  Stemming= False\n",
      "------------------------------------------------------------\n",
      "TERM            TOTAL  D1   D2   D3   D4   D5   D6   D7   D8\n",
      "the            42083 5178 8302 8767 3174 5833 2241 3794 4794\n",
      ".              28176 4818 4935 4228 2793 2803 1763 3832 3004\n",
      "of             19694 2421 3304 4322 2358 2370 1152 1466 2301\n",
      "and            19163 2358 2278 3240 2304 2121 1235 3124 2503\n",
      "a              15432 1968 2772 2719 1536 2092  815 1895 1635\n",
      "to             13276 1864 2056 1941 2239 1583  691 1727 1175\n",
      "in             10300 1118 1848 2206 1265 1370  537  955 1001\n",
      "it              8204 1035 2045  901 1105  703  418 1309  688\n",
      "i               7621  822   51    6 1282 1893 1265 1007 1295\n",
      "that            6840 1165 1115  785  805  722  433 1022  793\n",
      "was             6487 1137  198  118 1112 1335  552 1181  854\n",
      "is              6142  457 2056 2421  532  226  105  187  158\n",
      "not             4576  531  691  506 1044  431  157  958  258\n",
      "as              4470  543  745  900  684  488  261  398  451\n",
      ";               4399  299  979  633 1172  317  113  643  243\n",
      "he              4250 1390  177   39  544  299  122 1252  427\n",
      "you             4234  587 1485   34  918  120  125  884   81\n",
      "with            4226  574  625  525  663  527  215  648  449\n",
      "for             4010  495  436  529  726  726  217  533  348\n",
      "on              3831  366  797  388  411  976  137  378  378\n",
      "____________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "file_path = 'TextFiles/TextFiles/'\n",
    "files = ['T1.txt', 'T2.txt', 'T3.txt', 'T4.txt', 'T5.txt', 'T6.txt','T7.txt', 'T8.txt']\n",
    "term_doc = []\n",
    "pos_tags = False\n",
    "stemming = False\n",
    "remove_stop = False\n",
    "\n",
    "#Initialize and Reading File\n",
    "for file in files:\n",
    "    with open (file_path+file, \"r\") as text_file:\n",
    "        adoc = text_file.read()\n",
    "    # Convert to all lower case - required\n",
    "    adoc = (\"%s\" %adoc).lower()\n",
    "\n",
    "    # Replace special characters with spaces\n",
    "    adoc = adoc.replace('-', ' ')\n",
    "    adoc = adoc.replace('_', ' ')\n",
    "    adoc = adoc.replace(',', ' ')\n",
    "\n",
    "    # Replace not contraction with not\n",
    "    adoc = adoc.replace(\"'nt\", \" not\")\n",
    "    adoc = adoc.replace(\"n't\", \" not\")\n",
    "    adoc = adoc.replace(\"'d\", \" \")\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(adoc)\n",
    "    tokens = [word.replace(',', '') for word in tokens]\n",
    "    tokens = [word for word in tokens if ('*' not in word) and word != \"''\" and word !=\"``\"]\n",
    "\n",
    "    for word in tokens:\n",
    "        word = re.sub(r'[^\\w\\d\\s]+','',word)\n",
    "    print(\"\\nDocument \"+file+\" contains a total of\", len(tokens), \" terms.\")\n",
    "    \n",
    "    #POS Tagging\n",
    "    if pos_tags:\n",
    "        tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Remove stop words\n",
    "    if remove_stop:\n",
    "        stop = stopwords.words('english') + list(string.punctuation)\n",
    "        stop.append(\"said\")\n",
    "        # Remove single character words and simple punctuation\n",
    "        tokens = [word for word in tokens if len(word) > 1]\n",
    "        # Remove stop words\n",
    "        if pos_tags:\n",
    "            tokens = [word for word in tokens if word[0] not in stop]\n",
    "            tokens = [word for word in tokens if (not word[0].replace('.','',1).isnumeric()) and word[0]!=\"'s\" ]\n",
    "        else:\n",
    "            tokens = [word for word in tokens if word not in stop]\n",
    "            tokens = [word for word in tokens if word != \"'s\" ]\n",
    "            \n",
    "    # Lemmatization - Stemming with POS\n",
    "    if stemming:\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        wn_tags = {'N':wn.NOUN, 'J':wn.ADJ, 'V':wn.VERB, 'R':wn.ADV}\n",
    "        wnl = WordNetLemmatizer()\n",
    "        stemmed_tokens = []\n",
    "        if pos_tags:\n",
    "            for token in tokens:\n",
    "                term = token[0]\n",
    "                pos = token[1]\n",
    "                pos = pos[0]\n",
    "                try:\n",
    "                    pos = wn_tags[pos]\n",
    "                    stemmed_tokens.append(wnl.lemmatize(term, pos=pos))\n",
    "                except:\n",
    "                    stemmed_tokens.append(stemmer.stem(term))\n",
    "        else:\n",
    "            for token in tokens:\n",
    "                stemmed_tokens.append(stemmer.stem(token))\n",
    "    if stemming:\n",
    "        print(\"Document \"+file+\" contains\", len(stemmed_tokens), \"terms after stemming.\\n\")\n",
    "        tokens = stemmed_tokens\n",
    "        \n",
    "    #Prepare Counts & Add to term_doc\n",
    "\n",
    "    #fdist = FreqDist(word for word in stemmed_tokens)\n",
    "    fdist = FreqDist(tokens)\n",
    "    # Use with Wordnet\n",
    "    td= {}\n",
    "    #term_doc = []\n",
    "    for word, freq in fdist.most_common(2000):\n",
    "        td[word] = freq\n",
    "    term_doc.append(td)\n",
    "    \n",
    "#Prepare Term-Document Matrix\n",
    "\n",
    "td_mat = {}\n",
    "for td in term_doc:\n",
    "    td_mat = Counter(td_mat)+Counter(td)\n",
    "td_matrix = {}\n",
    "for k, v in td_mat.items():\n",
    "    td_matrix[k] = [v]\n",
    "\n",
    "for td in term_doc:\n",
    "    for k, v in td_matrix.items():\n",
    "        if k in td:\n",
    "            td_matrix[k].append(td[k])\n",
    "        else:\n",
    "            td_matrix[k].append(0)\n",
    "                \n",
    "#Print Term Document Matrix\n",
    "\n",
    "td_matrix_sorted = sorted(td_matrix.items(), key=operator.itemgetter(1),reverse=True)\n",
    "print(\"Scenario: POS=\", pos_tags, \"Remove Stop Words=\", remove_stop, \" Stemming=\", stemming)\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"TERM            TOTAL  D1   D2   D3   D4   D5   D6   D7   D8\")\n",
    "for i in range(20):\n",
    "    s = '{:<15s}'.format(td_matrix_sorted[i][0])\n",
    "    v = td_matrix_sorted[i][1]\n",
    "    #print(v)\n",
    "    for j in range(9):\n",
    "        s = s + '{:>5d}'.format(v[j])\n",
    "    print('{:<60s}'.format(s))\n",
    "print(\"____________________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that, this matrix doesnt result in a good analysis of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
